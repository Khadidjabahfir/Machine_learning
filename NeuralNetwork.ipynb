{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e31832fc-ff35-4a81-8a83-83611fab2531",
   "metadata": {},
   "source": [
    "# Neural Networks \n",
    "we have seen earlier that regression either linear or logistic could produce really good results in prediction however neural networks are better option sometimes since by adding hidden layers we will allow the neural networks to learn to consider some hidden features that it would be so tiring to do it manually. in neural network a neuron would be nothing more than one of learning algorithm we learnt earlier softmax , sigmoid , linear and in addition to the ones we have seen we would get to use a new one called relu. so we will start by defining the activation function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f631b7c-c4ae-4728-9e72-8db79c6af230",
   "metadata": {},
   "source": [
    "## activation functions \n",
    "there are three activation that we will be using in the neural network. we will try to add another one for the softmax later on. The three one implemented now are : relu => it takes the max within 0 and Z such that Z = W.X + B. Linear => it returns Z = W.X + B. Sigmoid => it returns sigmoid(Z = W.X + B). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c770ef7-377a-4642-8f12-a054094cb5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first import numpy \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35391d5f-9eb0-46d3-acd8-fb2d6865151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the activation functions \n",
    "def activationfunctions (activation,X,W,B) : \n",
    "    # X is a training example \n",
    "    # W is a weight parameter as a matrix n*number of neurons within a layer \n",
    "    # B is a bias the vector of number of neurons \n",
    "    if activation == \"linear\" : \n",
    "        return np.dot(X,W)+B # linear regression model \n",
    "    elif activation == \"relu\" : \n",
    "        return max(0,np.dot(X,W)+B) # relu model \n",
    "    elif activation == \"sigmoid\" : \n",
    "        return 1/(1+np.exp(-(np.dot(X,W)+B))) # logistic regression model \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274bdda4-683f-4bae-9783-34b25d63ade9",
   "metadata": {},
   "source": [
    "## Dense : \n",
    "we will try to define the dense function which represents the dense layer which would be a collection for neuron and then we will define the sequential which collects the layer into one big layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "603b0d42-9782-4962-8044-adc25e2e7277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense : \n",
    "    def __init__ (self, units, activation): \n",
    "        self.units = units # number of neurons \n",
    "        self.activation = activation # the type of activation function \n",
    "        self.Weights # weights of parameters \n",
    "        self.bias # bias parameters \n",
    "        self.activationVector = np.zeros(self.units) # the vector returned from the layer \n",
    "    def compile (self, X): \n",
    "        return activationfunctions(self.activation,X,self.Weights,self.bias) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9533a810-6785-4b39-a5d4-d2ca4d9cce6c",
   "metadata": {},
   "source": [
    "## Sequential  \n",
    "we will try to define a sequantial function. I will try to represent it as a class, and I will try an array of neurons as a parameter and I will set the weight and the bias to consider the output of the layer before and the number of neurons within the layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f639de6d-6a84-4b51-bf20-ef722eb3c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential: \n",
    "    def __init__(self, layers): \n",
    "        self.layers = layers \n",
    "        self.errorFunction \n",
    "    # a function to specify the error function used \n",
    "    def compile (self , error ) : \n",
    "        self.errorFunction = error \n",
    "    # here we will define a set of Helper functions \n",
    "    # starting with forward , backward and compute error \n",
    "    def forward (self ,X) : \n",
    "        # X is one training example \n",
    "        layers = self.layers \n",
    "        l = len (layers) # the number of layers \n",
    "        for i in range (l) :  \n",
    "            if i == 0 :\n",
    "                layers[0].activationVector= layers[0].compile(X) # for the first hidden layer \n",
    "            else : \n",
    "                layers[i].activationVector=layers[i].compile(layers[i-1].activationVector) # for the other hidden / output layer \n",
    "        return layers[l-1].activationVector # return the activation of the output layer \n",
    "    # define the error function \n",
    "    def error_function (self, X,Y): \n",
    "        # X is the training set _features_ of examples \n",
    "        # Y is the training set _outputs_ of examples \n",
    "        m = X.shape[0] \n",
    "        # loop over training examples \n",
    "        for i in range (m) : \n",
    "            \n",
    "     # define the error function depending on the loss function specified in the        \n",
    "        \n",
    "        \n",
    "    # define the backward function \n",
    "    def backward () : \n",
    "    \n",
    "        \n",
    "    def fit(self, X, Y , epoch ): \n",
    "        layers = self.layers \n",
    "        l = len(layers) # get the number of layers within the neural network \n",
    "        #initialize the parameters within the neural network \n",
    "        for i in range(l): \n",
    "            layer = layers[i]\n",
    "            if i == 0: \n",
    "                # intialize the parameters of the first layer randomaly taking into consideration both input layer and the number of neurons\n",
    "                layer.weights = np.random.randn(X.shape[1], layer.units)  # Input shape x units in the first layer\n",
    "                layer.bias = np.random.randn(layer.units)  # Initialize bias for the first layer\n",
    "            else: \n",
    "                # Initialize weights for the hidden/output layers\n",
    "                layer.weights = np.random.randn(layers[i - 1].units, layer.units)  # Previous layer's units x current layer's units\n",
    "                layer.bias = np.random.randn(layer.units)  # Initialize bias for the current layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc56cb-6dec-49bc-afe3-ee36c0573be3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
