{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ef9b42-b9c5-4c42-bba4-6819bec2ea85",
   "metadata": {},
   "source": [
    "# MultiClass Logistic Regression \n",
    "earlier, we have implemented binary classification however now, we will try to implement a multiclass logistic regression where we should consider the output to be more than 2 categories. thus we will try to define the model , loss function , lost function , run the gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a170fcd7-e4c7-4860-bc1f-337e2f9abbff",
   "metadata": {},
   "source": [
    "## Model \n",
    "now, you have more than two classes. You have C classes and for each class assign to it a number k between 1 to C. that doesn't reflect the class but it is only a representation of the class within the multiclass. y=1,...,C the output. for each output value we will have a parameter a vector w and b value.thus we will need to have a matrix of size n*C for the parameter W one vector for each class. we will also a vector of size C for the b each value for a class. we will compute the probabilty of y being a certain class taking into consideration the feature vector $P(y=j/{x})$. As usual, we will start by importing the numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca6baad-c41f-4ac3-8737-16aaf7c2605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc97b9e-2a3a-4696-bbc9-5b9044f8cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will define a function that returns a vector of Zi \n",
    "def linear_regression_for_multiclass (x,W,B,C) : \n",
    "    # x is a training example \n",
    "    # W is a matrix of parameters W\n",
    "    # B is a vector of parameter \n",
    "    # C is the number of classes \n",
    "    # we will return Z the output which is the vector \n",
    "    Z = np.zeros(C) # the output each Zi = wi.x + bi \n",
    "    for i in range (C): \n",
    "        Z[i] = np.dot(W[i],x)\n",
    "        Z[i] += B[i] \n",
    "    return Z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e0ecbc1-4be2-454e-bb6a-a257da929c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will define a function that is exp(Zi) \n",
    "def exponsential_multiclass (x,W,B,C) : \n",
    "    Z = linear_regression_for_multiclass(x,W,B,C) \n",
    "    Z = np.exp(Z) # compute the exp(Zi)\n",
    "    return Z # return the function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6beb53f6-527e-4dc3-ab4a-77d10cb4e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will define a function to compute ai \n",
    "def compute_activation_a (x,W,B,C) :  \n",
    "    Z = exponsential_multiclass (x,W,B,C) \n",
    "    return Z/Z.sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed6bd8c-c8b1-42f9-88fa-0db138331330",
   "metadata": {},
   "source": [
    "## lost function \n",
    "now we will define the loss function and then we will define the lost function, for the loss function , for it to be computed we should if y = i then the loss for the example is  : $-\\log{(a[i])}$ and then to define the lost function we will sum on all the losses of each training example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e63423a2-f60c-4f6a-b102-f41e385293a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss (x,W,B,C,y) :\n",
    "    # y is the output (one of classes)\n",
    "    # compute the a activation vector \n",
    "    a = compute_activation_a (x,W,B,C) \n",
    "    # y is the class that we are referring to \n",
    "    return -np.log(a[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "317045b8-facb-4010-9111-76e4b8be2e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lost (X,W,B,C,Y) : \n",
    "    # Y is the vector of outputs containing values from 0..C \n",
    "    # W is the matrix of parameters \n",
    "    # B is the vector of parameters \n",
    "    # C is the number of classes \n",
    "    # X is the set of training set \n",
    "    m = X.shape[1] \n",
    "    thesum = 0.0 \n",
    "    for i in range (m) : \n",
    "        iloss = loss (X[i],W,B,C,Y[i])\n",
    "        thesum += iloss \n",
    "    return thesum/m \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f1041-c841-4991-86f3-ec2a0037bde3",
   "metadata": {},
   "source": [
    "## Gradient Descent \n",
    "now, we will try to compute the gradient descent but we will try to compute before the gradient. I had to google how to compute the partial derivative in terms of W and B  : \n",
    "$\\frac{\\partial(L)}{\\partial(Wi)}=(ai -1i=y )*x $ and for computing the gardient in terms of b : \n",
    "$\\frac{\\partial(L)}{\\partial(Bi)}=(ai -1i=y )$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d472f18-3c63-4977-8c83-ea999b93be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the gradient \n",
    "def compute_gradient (X,Y,W,B,C): \n",
    "    \"\"\"\n",
    "    X: A set of training examples, shape (m,n) \n",
    "    Y: A set of outputs containing a value 0...C-1 \n",
    "    W: initial weights , shape (C,n) \n",
    "    B: initial biases , shape (C,).\n",
    "    \"\"\"\n",
    "    n = X.shape[1] # number of features \n",
    "    m = X.shape[0] # number of training examples \n",
    "    dW = np.zeros((C,n))\n",
    "    dB = np.zeros(C)\n",
    "    for i in range (m) : \n",
    "        x = X[i]\n",
    "        y = Y[i]\n",
    "        # compute softmax probabilities \n",
    "        a = compute_activation_a(x,W,B,C) \n",
    "        # compute the gradients for each class \n",
    "        for c in range (C) : \n",
    "            dW[c] += (a[c] - (c==y)) * x \n",
    "            dB[c] += (a[c] - (c==y))\n",
    "    # average the gradients \n",
    "    dW /= m \n",
    "    dB /= m \n",
    "    return dW,dB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3ffa52e-3b08-4789-a57e-7f1e14a16262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that applies gradient descent \n",
    "def Gradient_Descent (X,Y,W,B,C,alpha): \n",
    "    \"\"\"\n",
    "    X : is the set of training examples \n",
    "    Y : is the set of labels / outputs \n",
    "    W : is the matrix of weights \n",
    "    B : is the vector of biases \n",
    "    alpha : is the learning rate \n",
    "    \"\"\"\n",
    "    current_cost = lost(X,W,B,C,Y) # compute the cost with initial parameters \n",
    "    for i in range (1000) : \n",
    "        dW,dB = compute_gradient(X,Y,W,B,C) # compute the gradient \n",
    "        W = W - alpha * dW # update W\n",
    "        B = B - alpha * dB # update B \n",
    "        new_cost = lost(X,W,B,C,Y) # compute the current cost \n",
    "        if new_cost > current_cost : \n",
    "            break ; \n",
    "        else : \n",
    "            current_cost = new_cost \n",
    "    return W,B # return the values of W and B \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fd408f-fe35-47d8-9c77-9f7e90176d43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
